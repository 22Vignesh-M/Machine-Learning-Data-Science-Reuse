{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "b7a55317-1eee-4d79-b605-604089f5ba18", "_uuid": "81199409d9f6f6df20cc17e4e96406cd22158f3e"}, "source": ["Let's try to employ some tools for dimensionality reduction to our data a and see what comes out. In special, it is often  interesting to see what PCA tells us about the problem. NMF also frequently enconters useful \"parts\" to decompose our image, and always produces interesting results in datasets like MNIST or face analysis. We are specially interested here to see how these methods deal with one of the most basic facts about this dataset: sometimes we have one, and sometimes two dice."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "f31b468b-8196-40ed-afcf-d0c4806f6ee6", "_uuid": "dc3b40033d79ecfd73d6a594f2ac950191b31ab1"}, "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline   \n", "plt.rcParams['image.cmap'] = 'gray'\n", "\n", "def read_vectors(*filenames):\n", "    data = np.vstack(\n", "        tuple(np.fromfile(filename, dtype=np.uint8).reshape(-1,401)\n", "                      for filename in filenames))\n", "    return data[:,1:], data[:,0]\n", "\n", "X_train, y_train = read_vectors(*[\n", "    \"../input/snake-eyes/snakeeyes_{:02d}.dat\".format(nn) for nn in range(2)])\n", "X_test, y_test = read_vectors(\"../input/snake-eyes/snakeeyes_test.dat\")"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f6d1d4c1-cc3c-44a5-a33a-f4cb54b3c4f6", "_uuid": "d19378c698c0477e71746af80abe7019bbe14a6b"}, "source": ["We are basing this on https://www.kaggle.com/chientien/mnist-pca-nmf-lda-t-sne, who performed these analyses on MNIST. Thanks!\n", "\n", "# PCA\n", "\n", "Let's first look at the power of the first few principal components to \"explain\" the data."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "caa70b24-24ca-40e5-911e-b334345f4965", "_uuid": "a742eb9d89341b239d48fb2bedd8dea2e9c66662"}, "source": ["from sklearn.decomposition import PCA\n", "pca = PCA(random_state=0, whiten=True)\n", "pca.fit(X_train);"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "f32cceab-fd56-4e66-be8d-edb9a85e9588", "_uuid": "e486adaf209b14bc81de90e62dae4d3a18ee45b8"}, "source": ["exp_var_cum = np.cumsum(pca.explained_variance_ratio_)\n", "plt.plot(range(exp_var_cum.size), exp_var_cum)\n", "plt.grid()"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "1cf46835-d58f-4c3f-8063-760ce158f0b6", "_uuid": "0da4a9453a6d2e353fcf4f8eba8423a1bab7bde3"}, "source": ["plt.plot(range(exp_var_cum.size), exp_var_cum, '-+')\n", "plt.grid()\n", "plt.xlim(15,105)\n", "plt.ylim(0.6,0.95);"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b101177a-cc57-4a67-afce-ce87e68dfe2c", "_uuid": "0cc1a780da976a8e5ae26dc7f62473afd25fe838"}, "source": ["The result does not seem very different from MNIST. We could argue none of these image datasets can be _really_ explained by PCA, so neither of them are 100% explained by the e.g. first 10 components. This actually tell us something about the problem being very much non-convex, because our data does indeed fit very constrained manifolds, since most of the variation comes from the 3-DOF transforms over the dice faces. PCA does not seem to be able to grasp this fact, though, and this is because these \"thin\" manifolds are directed everywhere, and not just at a single direction that PCA would be able to find.\n", "\n", "Anyway, I wonder what these principal components look like..."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "824a3a5d-b63a-444b-b6dc-9f4d1577ae06", "_uuid": "7b06742695696cec7fcd9748c67f3d278680dc69"}, "source": ["plt.figure(figsize=(20,10))\n", "for k in range(40):\n", "    plt.subplot(4,10,k+1)\n", "    plt.imshow(pca.components_[k].reshape(20,20))\n", "    plt.axis('off')"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "2834f5f8-426f-407b-ae60-99fdfa7507bb", "_uuid": "8fbdf8005caec3f584d5646274e544fc34f14654"}, "source": ["Cool! Seems like we got ourselves some family of orthogonal basis functions here, something kind of like a Fourier transform or spheric Fourier transfom, who knows. Our very own Chladni patterns behind the good vibrations in this dataset. One may even wonder if instead of all the work and energy to calculate this from the data, we might not have just figured out these functions from formulas we found in a book that we read at school.\n", "\n", "It is interesting to notice that although it was hard to see any difference to MNIST in the curves above, if we look at the actual principal components, on MNIST they still look a bit like combinations of the digits somehow. The figure below shows how the first MNIST principal components look like.\n", "\n", "![MNIST principal components](https://i.imgur.com/9BGBKlj.png?1)\n", "\n", "Using your imagination you can easily see the contous of a \"0\" or a \"9\" in there, while in our case we get something that looks more like some generic basis. Maybe this is the first clue we have that MNIST and Snake Eyes, actually have some deep differences about them. \n", "\n", "Let's cut the PCA to the lucky 90 components (that reach 90% \"explainability\"), and try some NN. I mean nearest neighbors, not neural networks."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "9c7b6e7e-1b0d-40fa-9c98-1f284f813ab2", "_uuid": "4aa4a292384b58701d1a090f575e81fd40bf7973"}, "source": ["pca = PCA(n_components=90, random_state=0, whiten=True)\n", "pca.fit(X_train)\n", "X_train_pca = pca.transform(X_train)\n", "X_test_pca = pca.transform(X_test)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a864e288-2a8b-41eb-b192-c3be59ef794d", "_uuid": "f2cf7acc2548d43b55d17cf0ebbcf61e80863c29"}, "source": ["But first I wonder: how do the PCA reconstructions look like?"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "ddf1b4b8-71ad-4e2e-b0d0-ff59e8320c44", "_uuid": "4f7efa455cff70a90e57952c81634051decd0a54"}, "source": ["X_reconstructed_pca = pca.inverse_transform(X_test_pca)\n", "\n", "plt.figure(figsize=(20,10))\n", "for k in range(20):\n", "    plt.subplot(4, 10, k*2 + 1)\n", "    plt.imshow(X_test[k].reshape(20,20))\n", "    plt.axis('off')\n", "    plt.subplot(4, 10, k*2 + 2)\n", "    plt.imshow(X_reconstructed_pca[k].reshape(20,20))\n", "    plt.axis('off')"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "6d753364-3df2-4992-a102-4fb5b6dd4078", "_uuid": "a357f148cc51d08e6e2a2d1301735abcf515cc18"}, "source": ["It's kinf of ugly, perhaps? Although the shape of the dice is there, and we can read pretty much all of the faces. We seem to have what we want: being able to tell what is what with less variables, only 22.5% of the original feature vector size. I tried 40 components before, and the faces look blurry. You can try out to play with this parameter later, here we are mostly concerned with making cool graphics, and 90 looks cooler.\n", "\n", "But anyway, my opinion of how this data looks doesn't really matter, what is the opinion of NN?"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "21d2151d-ed3a-4fb0-b206-538e54c4d1a8", "_uuid": "c038075ba3e84645aa62ec807192db81010d2d9a"}, "source": ["KNN_PCA_TRAIN_SIZE = 200000\n", "KNN_PCA_TEST_SIZE = 200\n", "\n", "from sklearn.neighbors import KNeighborsClassifier\n", "temp = []\n", "for i in [1, 5]:\n", "    knn_pca = KNeighborsClassifier(n_neighbors=i, n_jobs=8)\n", "    knn_pca.fit(X_train_pca[:KNN_PCA_TRAIN_SIZE], y_train[:KNN_PCA_TRAIN_SIZE])\n", "    train_score_pca = knn_pca.score(X_train_pca[:KNN_PCA_TEST_SIZE], y_train[:KNN_PCA_TEST_SIZE])\n", "    test_score_pca = knn_pca.score(X_test_pca[:KNN_PCA_TEST_SIZE], y_test[:KNN_PCA_TEST_SIZE])\n", "    li = [i,train_score_pca,test_score_pca]\n", "    temp.append(li)\n", "temp"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d2e9a3fc-cbea-4180-a8dd-57a957dee02f", "_uuid": "5215a59123acf37941de48b26f1cf36827da80bc"}, "source": ["We limited the amount of data used in this experiment because NN inference takes a looong time to run. It's actually pretty much the only reason you might like to use it with the PCA transformed vectors instead of the original ones: to save on computation time. There is actually absolutely no guarantees that using PCA for this is a good idea, it is just a very desperate move. Every time you see someone applying PCA straight away for classification you should be sympathetic to this person, because they are most certainly desperate to run this classifier model on this data, but have a very constrained computational budget, and then see themselves forced to make this simplification that in many cases is actually bad for classification. Heck, we don't even know if Euclidean distance [is the good metric](http://yann.lecun.com/exdb/publis/pdf/simard-00.pdf) to employ here.\n", "\n", "Sometimes the information you are throwing away when you calculate the PCA is precisely the information that might help you classify the data, and not merely compress i.e. minimize the reconstruction error. This is what LDA is about, finding out the direction that really matters for classification, and not for reconstruction. These two can be very different problems, representation and classification. In fact, one might wonder if finding out this encoding that would let you do a good classification is not the ultimate goal of machine learning.\n", "\n", "I see now the NN test is over, and even though we did not use all our data, we can see an interesting result, we got 50% accuracy. Much better than the LDA [we tried before](https://www.kaggle.com/nicw102168/plotting-the-data-and-a-simple-demo). This shows the power of nearest neighbor, and how being more flexible than LDA is important for this problem. This is much more evident here than on MNIST, where LDA does not fare so bad.  But anyway, this NN seems quite slow. There are many things you can do to speed it up, though, but we will not look at this right now, let's stick to more basic analyses.\n", "\n", "## NMF\n", "\n", "If PCA is not such a terrific idea, perhaps we can try something else, something more non-linear, and something that has some good reasons to give good results in image analysis. Non-negative matrix factorization is just one such thing, and it's exciting because [according to some researchers](https://arxiv.org/abs/1509.05009) it even has some connections to Deep Learning, and Deep is exciting!\n", "\n", "Let's train this NMF."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "2de36fd0-8ce5-4663-bed2-94d36f561f17", "_uuid": "e4b073023157bbb020cc57556b4b30106a6c3047"}, "source": ["NMF_TRAIN_SIZE = 100000\n", "\n", "from sklearn.decomposition import NMF\n", "nmf = NMF(n_components=90, random_state=0)\n", "nmf.fit(X_train[:NMF_TRAIN_SIZE])"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1f6aa3e2-81e9-4ca0-a50e-82bb06109284", "_uuid": "ad37136e201b521286d4d8fc4d94a1285b5f1435"}, "source": ["NMF training does take a lot of time too, I must say, but is it good for anything? Fair warning: we cut back again on the number of samples, so this is all just to have a taste of how these things work. Hard to say what would be the performance (as in speed and accuracy) on the full dataset and throwing all the GPUs we have at the problem.\n", "\n", "Let's check again how the components look like. NMF is cool because it tends to find \"parts\" of things. That is the power of non-negativity. If you want to decompose your input into more \"local\" things than what we got with PCA, non-negativity is a very positive strategy!\n", "\n", "And our dataset clearly has \"parts\": we are simulating these dice that are actual two physical parts that can move and rotate independently, and maybe NMF can find this. Our basis should hypotetically look just like what you would need to represent a single dice anywhere, and then we should be able to represent the two dice with that, because it is just a linear mixture."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "1bdca77d-4bac-42a5-a4ad-c925b0f17ddb", "_uuid": "05c010f9ffd230d322e45f9000e59eab8f24f8ad"}, "source": ["plt.figure(figsize=(20,10))\n", "for k in range(40):\n", "    plt.subplot(4, 10, k + 1)\n", "    plt.imshow(nmf.components_[k].reshape(20,20))\n", "    plt.axis('off')"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "7ffddaf5-9c2b-47a5-9775-fac1e883aacd", "_uuid": "3acdd43eaccb087ea37cd2c2c612ff7de3d0c220"}, "source": ["Cool right? It does do something. If you use fewer components it clearly finds some large blobs the size of the dice. I suggest you experiment with it. With more components though, we find these tiny blobs moving around, and the rotated dice must be reconstructed from them somehow... It does look like parts of single dice, as we hypothesized earlier, nothing wide, the size of two dice or of the whole image.\n", "\n", "These blobs are so tiny, tough, that it might be fair to say they are just working pretty much like chubby pixels. So maybe NMF is not doing much more than finding a representation of less resolution here. But it is doing it in a much more structured way than PCA.\n", "\n", "Let's now see how the reconstructiion looks like."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "a2cbc796-13a2-442b-a37d-5e5c2fbf504f", "_uuid": "069084b820287b7c41016483399b98d78fe16588"}, "source": ["X_train_nmf = nmf.transform(X_train)\n", "X_test_nmf = nmf.transform(X_test)\n", "X_reconstructed = nmf.inverse_transform(X_test_nmf)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "678d3700-edae-4bc2-aebe-9b37734460ad", "_uuid": "a3d214ef2de8866781a646caa77ffa21fd89ab6f"}, "source": ["plt.figure(figsize=(20,10))\n", "for k in range(20):\n", "    plt.subplot(4, 10, k*2 + 1)\n", "    plt.imshow(X_test[k].reshape(20,20))\n", "    plt.axis('off')\n", "    plt.subplot(4, 10, k*2 + 2)\n", "    plt.imshow(X_reconstructed[k].reshape(20,20))\n", "    plt.axis('off')"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "423e7f32-83a4-4f53-9149-2f685882c9e7", "_uuid": "927a5fa2742f4e73f2e7d42a01d29218d310c338"}, "source": ["So now we have a black background, in contrast to what PCA delivers, and the dice faces are kind of blurry in a different way. Let's try this again at the same NN model and see if there is any difference."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "50dd8dbd-9bdf-4cb0-865e-877e58392e2d", "_uuid": "304c8f687bc755670124739c298b519e14d6693f"}, "source": ["KNN_NMF_TRAIN_SIZE = 200000\n", "KNN_NMF_TEST_SIZE = 200\n", "\n", "from sklearn.neighbors import KNeighborsClassifier\n", "temp = []\n", "for i in [1, 5]:\n", "    knn_nmf = KNeighborsClassifier(n_neighbors=i, n_jobs=8)\n", "    knn_nmf.fit(X_train_nmf[:KNN_NMF_TRAIN_SIZE], y_train[:KNN_NMF_TRAIN_SIZE])\n", "    train_score_nmf = knn_nmf.score(X_train_nmf[:KNN_NMF_TEST_SIZE], y_train[:KNN_NMF_TEST_SIZE])\n", "    test_score_nmf = knn_nmf.score(X_test_nmf[:KNN_NMF_TEST_SIZE], y_test[:KNN_NMF_TEST_SIZE])\n", "    li = [i,train_score_nmf,test_score_nmf]\n", "    temp.append(li)\n", "temp"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "bdea4dde-bd67-4d2d-8996-3c3a2c5d7bdc", "_uuid": "5b09660eddb6935e4d9e7245fcf9faba09f07a23"}, "source": ["We got 59% accuracy now. A fair improvement over PCA."]}], "metadata": {"language_info": {"file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "nbconvert_exporter": "python", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}