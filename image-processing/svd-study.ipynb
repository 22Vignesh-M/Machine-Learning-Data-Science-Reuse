{"metadata": {"language_info": {"version": "3.6.3", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "7b904708349ebe0acb103bb0c26eac16d1628d73", "_cell_guid": "4cdcd013-5da8-4863-bbe0-99434fac7936"}, "cell_type": "markdown", "source": ["A stroll through the neighborhood manifold\n", "====\n", "\n", "When we created the Snake-Eyes dataset, one of the ideas was to offer so many points that data augmentation might not be necessary to achieve great results. This is also one of the reasons the resolution is so small compared to other popular datasets. In this kernel we are going to make some tests to check if our sampling volume is really so large that it allows us to investigate the structure of the underlying 3- or 6-dimensional manifolds these 400-dimensional datapoints are theoretically lying upon.\n", "\n", "We are basically going to retrieve some nearest neighbors, and look at the rank of the covariance matrix, that should have rank three for single-dice images. The eigenvectors in a neighborhood should also look like the jacobian of the iamge relative to the parameters from the image synthesis, namely the position and orientation of the dice, apart from the face. It is interesting to compare what we are going to see here, the PCA from very similar images that are undergoing restricted forms of transformation, to the PCA from the whole dataset, that [we covered in another kernel](https://www.kaggle.com/nicw102168/trying-out-some-pca-nmf-and-knn)."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "604c6178dca7c7b744f6c6d2d56f26485800d076", "_cell_guid": "11e6c4a0-459e-4cdc-8f64-4167fc61cefe", "collapsed": true}, "cell_type": "code", "source": ["import numpy as np\n", "import numpy.linalg\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline   \n", "plt.rcParams['image.cmap'] = 'gray'"]}, {"metadata": {"_uuid": "30f9bc166f414b8bb61b2116ba4a648a68309b6b", "_cell_guid": "7ad34925-8370-4d34-a346-3d4447348050"}, "cell_type": "markdown", "source": ["This is a generator to read our data outputting a single image each time. This is useful if you can't fit all the data in your computer's memory."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "082550e0fbdc42616fe896e4a007810885046570", "_cell_guid": "b66ce207-8784-4ed6-8ade-91433f49e58d", "collapsed": true}, "cell_type": "code", "source": ["def image_generator(*filenames):\n", "    for filename in filenames:\n", "        with open(filename, 'rb') as fp:\n", "            for _ in range(100000):\n", "                yield np.array(np.fromstring(fp.read(401), dtype=np.uint8)[1:], dtype=float)\n", "\n", "def img_gen():\n", "    return image_generator(*[\"../input/snake-eyes/snakeeyes_{:02d}.dat\".format(nn) for nn in range(10)])"]}, {"metadata": {"_uuid": "9a9ae5b17fbe3e3e5110542dbd7072f791555b1e", "_cell_guid": "673ce92f-8f9e-4f0e-ab90-045de1aae157"}, "cell_type": "markdown", "source": ["Just a couple of functions to diplay our nice dice images."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "6f3266d01ee4133b12d4309d2e84ebb101396980", "_cell_guid": "47cdab41-85b4-4a63-a6a7-892059d647e8", "collapsed": true}, "cell_type": "code", "source": ["def plotdice(x):\n", "    plt.imshow(x.reshape(20,20))\n", "    plt.axis('off')                \n", "def plotdicez(x):\n", "    plt.imshow(x.reshape(20,20), cmap=plt.cm.RdBu, vmin=-128, vmax=128)\n", "    plt.axis('off')"]}, {"metadata": {"_uuid": "3ff533cd3564837370daf65060fa7084a74e1fed", "_cell_guid": "ae1ae6ff-2540-4312-9128-07c30f066fee"}, "cell_type": "markdown", "source": ["Let's pick the first 6 images in the dataset to use as references."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "bd006158d4a85369e36d0e1f850c717c3a731e74", "_cell_guid": "f5aa624a-071a-44f3-914a-e35c499b27a1", "collapsed": true}, "cell_type": "code", "source": ["refs = np.array([x for _, x in zip(range(6), img_gen())])\n", "\n", "for n in range(6):\n", "    plt.subplot(1,6,n+1)\n", "    plotdice(refs[n])"]}, {"metadata": {"_uuid": "168c0b7125c5bdec3d835cfb4652c9c0f9b8bd64", "_cell_guid": "9ca3007d-7e00-42ff-aec9-1c087c5af6e7"}, "cell_type": "markdown", "source": ["Neat. Now we'll do the big-data step and go through all of our dataset looking for matching images. And notice our matching criterion is pretty tight: We are not looking for something below a certain euclidean distance, for instance. We want images that have a maximum difference (128) over all of the pixels. That means if a pixel is 255 in our reference image, we can tolerate at most it getting down to 127. A white pixel cannot trurn actual black, or vice-versa.\n", "\n", "We are really looking for images that look very much alike, that would mean they sit closely together in the manifold that contains the images for that given class/dice face. It would also mean the original parameters used to synthesize the image (position and rotation) should also be very close."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "33b10ccacf100b6162897fe0051cb4d00833be27", "_cell_guid": "b8e7458d-c0e9-429a-98ff-588cc6e2ea57", "collapsed": true}, "cell_type": "code", "source": ["query = [ww for ww in img_gen() if (np.max(np.abs(refs - ww), axis=1) < 128).any()]"]}, {"metadata": {"_uuid": "70d454e16f1a2235b79fa9c7908d40c6c85c0e82", "_cell_guid": "846de7b0-9bbb-4c36-bfde-127a764d2a96"}, "cell_type": "markdown", "source": ["Let's pick from our cache just the images relative to our first reference one."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "7e5251f21df43ef9d5337c801d1eae3f7c98b8c8", "_cell_guid": "08fe7773-f8f5-4d9e-a191-5f7894ad5cd0", "collapsed": true}, "cell_type": "code", "source": ["qq = query[0]\n", "\n", "sim = [ww for ww in query if np.max(np.abs((qq - ww))) < 128][1:]\n", "\n", "plt.figure(figsize=(8,6))\n", "for k, ww in enumerate(sim):\n", "    if k >= 20:\n", "        break\n", "    plt.subplot(4, 5, k + 1)\n", "    plotdice(ww)"]}, {"metadata": {"_uuid": "461d176591368140fba05bdedb3c1c039d1b85bc", "_cell_guid": "106c6622-6180-4030-a39b-614e24511bf2"}, "cell_type": "markdown", "source": ["It seems we got what we wanted, all images look pretty similar. And we got 20 of them, hopefully this will really serve to our purposes of studying this manifold where this data lives.\n", "\n", "Because this is a single dice image, apart from the possibility of changing the face, there are only three parameters controlling this image appearence, 2 related to the translation and 1 to rotation. That would mean this neighborhood of images should be sitting in a 3-dimensional manifold. That would mean in turn that if we look at the covariange matrix of these images it should have rank-3, and its eigenvectors should look like the derivatives related to the three degrees of freedom. Let's first look at the differences from each image to our reference image (that should be the \"mean\" from all of them)."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "d654a0af3cf194303bd70ff34139b462eba1a9f0", "_cell_guid": "b8556910-066f-4998-881a-6c1e3b6238bf", "collapsed": true}, "cell_type": "code", "source": ["plt.figure(figsize=(8,6))\n", "for k, ww in enumerate(sim):\n", "    if k >= 20:\n", "        break\n", "    plt.subplot(4, 5, k + 1)\n", "    plotdicez(ww - qq)"]}, {"metadata": {"_uuid": "3977d479841a7b3b2c1361bbefb8c5959d55b3bd", "_cell_guid": "009e1afe-4d4f-4be3-9d96-46374d5a4e7b"}, "cell_type": "markdown", "source": ["We can see the differences are mostly over the edges of the image, and they have some kind of structure. The dice is always kind of moving to a certain direction, making the opposite edges red and blue, for instance. Let's look now at the singular values from our covariance matrix."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "2e38102c68739f405913331545638522be0a1835", "_cell_guid": "475a4290-e79f-4efa-a158-9dff8189a566", "collapsed": true}, "cell_type": "code", "source": ["u, s, v = np.linalg.svd(np.cov(np.array(sim).T))\n", "\n", "plt.figure()\n", "plt.plot(s[:40], '-+')\n", "plt.grid()"]}, {"metadata": {"_uuid": "51e82d596965ca8ae895e616b5baf571f413d7d3", "_cell_guid": "e3e57960-2944-4919-9eeb-5d475cc55da0"}, "cell_type": "markdown", "source": ["Indeed, we got quite a drop after the third singular value, so apparently our theory was right!... Let's look now at the eigenvectors."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "19b03d40487ee403b82eb71e7a03a0cd29cc897c", "_cell_guid": "0332671d-0785-4a0e-a8cb-af88c8fe7345", "collapsed": true}, "cell_type": "code", "source": ["plt.figure()\n", "for n in range(3):\n", "    plt.subplot(1, 3, n + 1)\n", "    plotdicez(v[n] * 400)"]}, {"metadata": {"_uuid": "a49c00829266b132607bf19c3de3b9b896a51cf0", "_cell_guid": "aa093077-0d05-4fe9-b846-32eac9840631"}, "cell_type": "markdown", "source": ["I would say the first one kind of models a rotation mostly, and the other two model the translations... Let's do the same analysis on other reference images and see what comes out."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "b68dbbc6578caea85f5b22328c3f594ea9812541", "_cell_guid": "201252cf-b4fa-4463-8a33-380ab6ad8de0", "collapsed": true}, "cell_type": "code", "source": ["qq = query[1]\n", "\n", "sim = [ww for ww in query if np.max(np.abs((qq - ww))) < 128][1:]\n", "\n", "plt.figure(figsize=(20,8))\n", "for k, ww in enumerate(sim):\n", "    if k >= 20:\n", "        break\n", "    plt.subplot(4, 10, k//5*10 + k%5 + 1)\n", "    plotdice(ww)\n", "    \n", "for k, ww in enumerate(sim):\n", "    if k >= 20:\n", "        break\n", "    plt.subplot(4, 10, k//5*10 + k%5 + 6)\n", "    plotdicez(ww - qq)\n", "\n", "u, s, v = np.linalg.svd(np.cov(np.array(sim).T))\n", "\n", "plt.figure()\n", "plt.plot(s[:40], '-+')\n", "plt.grid()\n", "\n", "plt.figure()\n", "for n in range(3):\n", "    plt.subplot(1, 3, n + 1)\n", "    plotdicez(v[n] * 400)"]}, {"metadata": {"_uuid": "e2d6b546abaa1af29df8b42a418eaf4a1be1f970", "_cell_guid": "fbeb638f-0286-46b1-af63-f7158a95de2e"}, "cell_type": "markdown", "source": ["Similar result. I would say it is easier to see in this case the second image is mostly modeling the rotation. You can see that the top left edge changes from blue to red, and the color around the two dots are kind of inverted, what would only be possible rotating the dice. On the other images we quite clearly have either blue or red edges, and there are no opposite edges with the same color, that is what you would expect for components modeling translation.\n", "\n", "Let's look at other reference images."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "a63e117f9a3fa9931cb843d64841d947a6b7e6c1", "_cell_guid": "7a873f1f-4708-421d-b33b-96f1d418536f", "collapsed": true}, "cell_type": "code", "source": ["qq = query[2]\n", "sim = [ww for ww in query if np.max(np.abs((qq - ww))) < 128][1:]\n", "len(sim)"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "80fd86fdf32e17693356f8f6a5f475f12534fb7c", "_cell_guid": "8685bc75-9bf1-4e09-9574-a29db65bff26", "collapsed": true}, "cell_type": "code", "source": ["qq = query[3]\n", "sim = [ww for ww in query if np.max(np.abs((qq - ww))) < 128][1:]\n", "len(sim)"]}, {"metadata": {"_uuid": "ef962446c2a10f5361547c944d82fb113f40010a", "_cell_guid": "4697fa76-275f-4292-8637-ddaba57a88b8"}, "cell_type": "markdown", "source": ["For these two, which were images with two dice, the query did not return a single neighboring image!! Even thogh we got more than 20 hits for a since dice, with two dice this seems too restrictive. I didn't quite expect such a huge difference from the two cases, I must confess. Let's see the other single-dice references."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "ead8c63c9369670faea98c16e2d78fc4a6729838", "_cell_guid": "3ba2e5cc-fb2d-48cc-ae95-d557f60d8d57", "collapsed": true}, "cell_type": "code", "source": ["qq = query[4]\n", "\n", "sim = [ww for ww in query if np.max(np.abs((qq - ww))) < 128][1:]\n", "\n", "plt.figure(figsize=(20,8))\n", "for k, ww in enumerate(sim):\n", "    if k >= 20:\n", "        break\n", "    plt.subplot(4, 10, k//5*10 + k%5 + 1)\n", "    plotdice(ww)\n", "    \n", "for k, ww in enumerate(sim):\n", "    if k >= 20:\n", "        break\n", "    plt.subplot(4, 10, k//5*10 + k%5 + 6)\n", "    plotdicez(ww - qq)\n", "\n", "u, s, v = np.linalg.svd(np.cov(np.array(sim).T))\n", "\n", "plt.figure()\n", "plt.plot(s[:40], '-+')\n", "plt.grid()\n", "\n", "plt.figure()\n", "for n in range(3):\n", "    plt.subplot(1, 3, n + 1)\n", "    plotdicez(v[n] * 400)"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "3078e83661b6bc1d906c51f85c2db0d799bcfa80", "_cell_guid": "901c26b0-961a-45b3-8895-91c5f49e1443", "collapsed": true}, "cell_type": "code", "source": ["qq = query[5]\n", "\n", "sim = [ww for ww in query if np.max(np.abs((qq - ww))) < 128][1:]\n", "\n", "plt.figure(figsize=(20,8))\n", "for k, ww in enumerate(sim):\n", "    if k >= 20:\n", "        break\n", "    plt.subplot(4, 10, k//5*10 + k%5 + 1)\n", "    plotdice(ww)\n", "    \n", "for k, ww in enumerate(sim):\n", "    if k >= 20:\n", "        break\n", "    plt.subplot(4, 10, k//5*10 + k%5 + 6)\n", "    plotdicez(ww - qq)\n", "\n", "u, s, v = np.linalg.svd(np.cov(np.array(sim).T))\n", "\n", "plt.figure()\n", "plt.plot(s[:40], '-+')\n", "plt.grid()\n", "\n", "plt.figure()\n", "for n in range(3):\n", "    plt.subplot(1, 3, n + 1)\n", "    plotdicez(v[n] * 400)"]}, {"metadata": {"_uuid": "789c16dca393fcf67486c84533d24c6ead0ca72a", "_cell_guid": "2ff6b27b-e6a0-42fd-8fba-75d6cc9da6ac"}, "cell_type": "markdown", "source": ["# Conclusion\n", "One of the principles we had in mind when creating the Snake-Eyes dataset was to make sure there was so many sampled instances compared to the image resolution that we would really be able to anlayze the manifolds where these datapoints lie upon. The manifolds are a consequence of the fact the images are created from a parametric model with three degrees of freedom. Our experiments demonstrated that given a certain distance threshold, we are capable of retrieving quite a large number of neighboring images, and this neighborhood does seem to be consistent with the fact these 400-dimension data points should live in a rank-3 manifold.\n", "\n", "The results concern only the single-dice images, though. For the 2-dice case, the sample volume wasn't enough to achieve the same result. Two-dice images should result in a 6-dimensional manifold. We still must check wether a less restrictive nearest-neighborhood query allow us to retrieve a sample where this property can be tested.\n", "\n", "Another future analysis we have in mind is to try to estimate these manifold dimensionalities using techniques such as box-counting. We also plan to investigate what happens when you throw this data in random projection trees, or utilize other dimensionality reduction techniques."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "6aaf27f35c6fa2703ad77e4da87edee2809743be", "_cell_guid": "d3110a27-f14d-4b4d-b4a0-388584141748", "collapsed": true}, "cell_type": "code", "source": []}], "nbformat": 4}